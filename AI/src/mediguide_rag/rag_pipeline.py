# rag_chain.py
import os
from dotenv import load_dotenv
from langchain_ibm import WatsonxLLM, WatsonxEmbeddings
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from ibm_watsonx_ai.metanames import EmbedTextParamsMetaNames

load_dotenv()

# ì „ì—­ ì„¤ì •
IBM_URL = os.getenv('IBM_CLOUD_URL')
PROJECT_ID = os.getenv('PROJECT_ID')
WATSONX_API = os.getenv('API_KEY')
PERSIST_DIR = "./chroma_db_fixed"

def get_rag_chain():
    # 1. ì„ë² ë”© ì„¤ì • (ingest.pyì™€ ë™ì¼í•´ì•¼ í•¨)
    embed_params = {
        EmbedTextParamsMetaNames.TRUNCATE_INPUT_TOKENS: 512,
        EmbedTextParamsMetaNames.RETURN_OPTIONS: {"input_text": True},
    }
    embeddings = WatsonxEmbeddings(
        model_id="ibm/granite-embedding-278m-multilingual",
        url=IBM_URL,
        project_id=PROJECT_ID,
        params=embed_params,
        apikey=WATSONX_API
    )

    # 2. ì €ì¥ëœ DB ë¡œë“œ (ë°ì´í„° ìƒì„± X)
    vectorstore = Chroma(
        persist_directory=PERSIST_DIR,
        embedding_function=embeddings
    )
    retriever = vectorstore.as_retriever(search_kwargs={'k': 3})

    # 3. LLM ì„¤ì •
    llm = WatsonxLLM(
        model_id="meta-llama/llama-3-3-70b-instruct",
        url=IBM_URL,
        apikey=WATSONX_API,
        project_id=PROJECT_ID,
        params={
            "decoding_method": "greedy",
            "max_new_tokens": 1000,
            "min_new_tokens": 1,
            "temperature": 0.1,
            # [í•µì‹¬ ìˆ˜ì •] AIì—ê²Œ "ì—¬ê¸°ì„œ ë©ˆì¶°!"ë¼ê³  ì•Œë ¤ì£¼ëŠ” ì‹ í˜¸ì…ë‹ˆë‹¤.
            # "ì§ˆë¬¸:" ì´ë¼ëŠ” ë‹¨ì–´ê°€ ë˜ ë‚˜ì˜¤ë ¤ê³  í•˜ë©´ ê°•ì œë¡œ ì…ì„ ë‹¤ë¬¼ê²Œ í•©ë‹ˆë‹¤.
            "stop_sequences": ["\nì§ˆë¬¸:", "\n\nì§ˆë¬¸:", "ì§ˆë¬¸:"]
        }
    )

    # 4. í”„ë¡¬í”„íŠ¸ & ì²´ì¸
    template = """ë‹¹ì‹ ì€ ì˜ë£Œ ë¶„ìŸ ìƒë‹´ AIì…ë‹ˆë‹¤. 
    ì•„ë˜ì˜ [ì°¸ê³  íŒë¡€]ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.
    ë‹µë³€ì„ ë§ˆì¹œ í›„ì—ëŠ” ì ˆëŒ€ë¡œ ìƒˆë¡œìš´ ì§ˆë¬¸ì„ ìƒì„±í•˜ì§€ ë§ê³  ì¢…ë£Œí•˜ì„¸ìš”.
    
    [ì°¸ê³  íŒë¡€]:
    {context}

    ì§ˆë¬¸: {question}
    
    ë‹µë³€:"""
    
    prompt = ChatPromptTemplate.from_template(template)

    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)

    rag_chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )
    
    return rag_chain

if __name__ == "__main__":
    print("ğŸ§ª RAG ë¡œì§ í…ŒìŠ¤íŠ¸ ì¤‘...")
    
    
    
    
    try:
        chain = get_rag_chain()
        test_query = "ë°±ë‚´ì¥ ìˆ˜ìˆ  í›„ ë¹› ë²ˆì§ ë¶€ì‘ìš©ì´ ìˆì–´"
        
        print(f"\nâ“ ì§ˆë¬¸: {test_query}")
        answer = chain.invoke(test_query)
        print(f"\nğŸ’¡ ë‹µë³€:\n{answer}")
        print("\nâœ… ë¡œì§ í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
    except Exception as e:
        print(f"\nâŒ ì—ëŸ¬ ë°œìƒ: {e}")