# rag_pipeline.py (Production-grade patch: Anti-hallucination + Anti-infinite-interview + Safe fallback + A/B/C)
import os
import json
import re
from typing import List, Tuple, Dict, Any, Optional

from dotenv import load_dotenv

from langchain_ibm import WatsonxLLM, WatsonxEmbeddings
from langchain_chroma import Chroma

from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableLambda, RunnableMap
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory

from ibm_watsonx_ai.metanames import EmbedTextParamsMetaNames

load_dotenv()

# ---------------------------------------------------------------------
# Global memory store (session -> chat history)
# ---------------------------------------------------------------------
store: Dict[str, ChatMessageHistory] = {}

# (ë¬¸ì§„ ë¬´í•œ ë£¨í”„ ë°©ì§€ìš©) session -> interview turn count
_interview_turns: Dict[str, int] = {}

# ---------------------------------------------------------------------
# Env
# ---------------------------------------------------------------------
IBM_URL = os.getenv("IBM_CLOUD_URL")
PROJECT_ID = os.getenv("PROJECT_ID")
WATSONX_API = os.getenv("API_KEY")

PERSIST_DIR = "./chroma_db_fixed"
COLLECTION_NAME = os.getenv("CHROMA_COLLECTION", "mediguide_cases")

# Model IDs (envë¡œ êµì²´ ê°€ëŠ¥)
MAIN_LLM_ID = os.getenv("MAIN_LLM_ID", "meta-llama/llama-3-405b-instruct")
RERANK_LLM_ID = os.getenv("RERANK_LLM_ID", "ibm/granite-3-8b-instruct")
ROUTER_LLM_ID = os.getenv("ROUTER_LLM_ID", "ibm/granite-3-8b-instruct")
WRITER_LLM_ID = os.getenv("WRITER_LLM_ID", "meta-llama/llama-3-405b-instruct")

# ---------------------------------------------------------------------
# Retrieval knobs (A: score gate / B: rerank)
# ---------------------------------------------------------------------
CANDIDATE_K = 25
FINAL_K = 5

# distance(ë‚®ì„ìˆ˜ë¡ ìœ ì‚¬) ê°€ì •. í™˜ê²½ì— ë”°ë¼ íŠœë‹ í•„ìš”.
MAX_DISTANCE_THRESHOLD = float(os.getenv("MAX_DISTANCE_THRESHOLD", "0.45"))

MAX_CONTEXT_CHARS_PER_DOC = 1400

# ë¬¸ì§„ ìµœëŒ€ í„´(ì„¸ì…˜ ë‹¹)
MAX_INTERVIEW_TURNS = int(os.getenv("MAX_INTERVIEW_TURNS", "2"))

# ---------------------------------------------------------------------
# Session history
# ---------------------------------------------------------------------
def get_session_history(session_id: str) -> BaseChatMessageHistory:
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    if session_id not in _interview_turns:
        _interview_turns[session_id] = 0
    return store[session_id]


# ---------------------------------------------------------------------
# Embeddings + VectorStore
# ---------------------------------------------------------------------
def _build_embeddings() -> WatsonxEmbeddings:
    embed_params = {
        EmbedTextParamsMetaNames.TRUNCATE_INPUT_TOKENS: 512,
        EmbedTextParamsMetaNames.RETURN_OPTIONS: {"input_text": True},
    }
    return WatsonxEmbeddings(
        model_id="ibm/granite-embedding-278m-multilingual",
        url=IBM_URL,
        project_id=PROJECT_ID,
        params=embed_params,
        apikey=WATSONX_API,
    )


def _build_vectorstore(embeddings: WatsonxEmbeddings) -> Chroma:
    return Chroma(
        persist_directory=PERSIST_DIR,
        embedding_function=embeddings,
        collection_name=COLLECTION_NAME,
    )


# ---------------------------------------------------------------------
# (B) Re-ranker LLM
# ---------------------------------------------------------------------
def _build_rerank_llm() -> WatsonxLLM:
    return WatsonxLLM(
        model_id=RERANK_LLM_ID,
        url=IBM_URL,
        apikey=WATSONX_API,
        project_id=PROJECT_ID,
        params={
            "decoding_method": "greedy",
            "max_new_tokens": 120,
            "min_new_tokens": 1,
            "repetition_penalty": 1.0,
            "stop_sequences": ["\n\n", "</s>", "<|end_of_text|>"],
        },
    )


def answer_with_sources(question: str, session_id: str = "default_user") -> Dict[str, Any]:
    """
    main.pyì—ì„œ 'ê·¼ê±° ë¶ˆì¼ì¹˜'ë¥¼ ì—†ì• ê¸° ìœ„í•œ ë‹¨ì¼ ì§„ì‹¤ ì†ŒìŠ¤.
    - rag_chainì´ ë§Œë“  ìµœì¢… ë‹µë³€ê³¼,
    - ê·¸ ë‹µë³€ì— ì‚¬ìš©ëœ ìµœì¢… rerank docsë¥¼ í•¨ê»˜ ë°˜í™˜.

    return:
      {
        "answer": str,
        "mode": "SOLUTION"|"INTERVIEW",
        "docs": List[Document]
      }
    """
    # âœ… ì„¸ì…˜ íˆìŠ¤í† ë¦¬ í™•ë³´
    history = get_session_history(session_id)

    # âœ… ì—¬ê¸°ì„œ get_rag_chain()ì„ ë§¤ë²ˆ ìƒˆë¡œ ë§Œë“¤ë©´ ëŠë ¤ì§ˆ ìˆ˜ ìˆìœ¼ë‹ˆ
    #    ê¸°ì¡´ main.pyì—ì„œ ìƒì„±í•œ rag_chainì„ ì“°ëŠ” ê²Œ ì´ìƒì ì´ì§€ë§Œ,
    #    êµ¬ì¡°ìƒ ì—¬ê¸°ì„œëŠ” ì²´ì¸ì„ í•œ ë²ˆ ìƒì„±í•´ì„œ ì‚¬ìš©.
    chain = get_rag_chain()

    # chainì€ ë‚´ë¶€ì ìœ¼ë¡œ retrieval_stepì—ì„œ mode/docs/contextë¥¼ ë§Œë“¤ê³ ,
    # route_and_answerì—ì„œ ë¬¸ìì—´ì„ ë°˜í™˜í•œë‹¤.
    #
    # BUT: í˜„ì¬ get_rag_chain() êµ¬í˜„ì€ ìµœì¢…ì ìœ¼ë¡œ "ë¬¸ìì—´"ë§Œ ë°˜í™˜í•˜ê³  ìˆì–´ docsë¥¼ ë°–ìœ¼ë¡œ ëª» êº¼ëƒ„.
    # ê·¸ë˜ì„œ ì•„ë˜ ë°©ì‹ìœ¼ë¡œ "ë™ì¼ ë¡œì§"ì„ í•œ ë²ˆ ë” ìˆ˜í–‰í•´ì„œ docsë¥¼ ë§Œë“¤ê³ , ë‹µë³€ì€ chainì„ ì‚¬ìš©í•œë‹¤.
    #
    # (ìµœê³  ì™„ì„±í˜•ì€ get_rag_chain ë‚´ë¶€ì—ì„œ docsë¥¼ í•¨ê»˜ ë°˜í™˜í•˜ë„ë¡ êµ¬ì¡°ë¥¼ ë°”ê¾¸ëŠ” ê²ƒ)
    embeddings = _build_embeddings()
    vectorstore = _build_vectorstore(embeddings)
    rerank_llm = _build_rerank_llm()

    # 1) í›„ë³´ ê²€ìƒ‰ + ê²Œì´íŠ¸
    pairs = _retrieve_candidates_with_scores(vectorstore, question, k=CANDIDATE_K)
    docs = [d for d, _ in pairs]
    scores = [s for _, s in pairs]

    if not _passes_gate(scores):
        mode = "INTERVIEW"
        final_docs: List[Document] = []
    else:
        mode = "SOLUTION"
        final_docs = _rerank_docs(rerank_llm, question, docs, top_n=FINAL_K)

    # 2) ë‹µë³€ ìƒì„±(ì„¸ì…˜ ë©”ëª¨ë¦¬ ì—…ë°ì´íŠ¸ëŠ” RunnableWithMessageHistoryê°€ ìˆ˜í–‰)
    answer = chain.invoke(
        {"question": question},
        config={"configurable": {"session_id": session_id}},
    )

    return {"answer": answer, "mode": mode, "docs": final_docs}
# ---------------------------------------------------------------------
# Main answer LLM
# ---------------------------------------------------------------------
def _build_main_llm() -> WatsonxLLM:
    return WatsonxLLM(
        model_id=MAIN_LLM_ID,
        url=IBM_URL,
        apikey=WATSONX_API,
        project_id=PROJECT_ID,
        params={
            "decoding_method": "greedy",
            "max_new_tokens": 900,
            "min_new_tokens": 10,
            "repetition_penalty": 1.08,
            "stop_sequences": ["<|end_of_text|>", "\n\nì§ˆë¬¸:", "User:"],
        },
    )


# ---------------------------------------------------------------------
# Utilities
# ---------------------------------------------------------------------
def _norm_text(x: str) -> str:
    if x is None:
        return ""
    x = str(x).replace("\r\n", "\n").replace("\r", "\n")
    x = re.sub(r"[ \t]+", " ", x)
    x = re.sub(r"\n{3,}", "\n\n", x)
    return x.strip()


def _safe_int_list_from_json(text: str) -> List[int]:
    text = (text or "").strip()
    try:
        data = json.loads(text)
        if isinstance(data, list) and all(isinstance(i, int) for i in data):
            return data
    except Exception:
        pass

    nums = re.findall(r"\d+", text)
    return [int(n) for n in nums][:FINAL_K]


def _format_docs_for_context(docs: List[Document]) -> str:
    """
    (C) [ê·¼ê±° n] í¬ë§· ê°•ì œ. case_id ë…¸ì¶œ ê¸ˆì§€.
    """
    if not docs:
        return ""

    blocks = []
    for i, d in enumerate(docs, 1):
        title = d.metadata.get("title", "ì œëª© ì—†ìŒ")
        dept = d.metadata.get("dept", d.metadata.get("medical_dept", "ì§„ë£Œê³¼ ì—†ìŒ"))
        section = d.metadata.get("section", "section ì—†ìŒ")
        seq = d.metadata.get("seq", "")

        body = _norm_text(d.page_content or "")
        if len(body) > MAX_CONTEXT_CHARS_PER_DOC:
            body = body[:MAX_CONTEXT_CHARS_PER_DOC] + "..."

        header = f"[ê·¼ê±° {i}] ì‚¬ê±´ëª…: {title} | ì§„ë£Œê³¼: {dept} | ì„¹ì…˜: {section}"
        if seq:
            header += f" | ì›ë¬¸ë²ˆí˜¸: {seq}"

        blocks.append(f"{header}\n{body}")

    return "\n\n".join(blocks)


# ---------------------------------------------------------------------
# (A) Score-gated retrieval + (B) rerank
# ---------------------------------------------------------------------
def _retrieve_candidates_with_scores(
    vectorstore: Chroma, query: str, k: int = CANDIDATE_K
) -> List[Tuple[Document, float]]:
    return vectorstore.similarity_search_with_score(query, k=k)


def _passes_gate(scores: List[float]) -> bool:
    if not scores:
        return False
    return min(scores) <= MAX_DISTANCE_THRESHOLD


def _rerank_docs(
    rerank_llm: WatsonxLLM, query: str, docs: List[Document], top_n: int = FINAL_K
) -> List[Document]:
    if not docs:
        return []

    snippets = []
    for idx, d in enumerate(docs):
        title = d.metadata.get("title", "ì œëª© ì—†ìŒ")
        section = d.metadata.get("section", "section ì—†ìŒ")
        dept = d.metadata.get("dept", d.metadata.get("medical_dept", ""))
        text = _norm_text(d.page_content or "")
        text = text[:500] + ("..." if len(text) > 500 else "")
        snippets.append(f"{idx}. (ì‚¬ê±´ëª…: {title} | ì§„ë£Œê³¼: {dept} | ì„¹ì…˜: {section}) {text}")

    rerank_prompt = f"""
ë‹¹ì‹ ì€ ê²€ìƒ‰ ê²°ê³¼ ì¬ì •ë ¬(rerank) ëª¨ë¸ì…ë‹ˆë‹¤.

ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê°€ì¥ ê´€ë ¨ì„±ì´ ë†’ì€ ë¬¸ì„œ ì¸ë±ìŠ¤ {top_n}ê°œë¥¼ ê³¨ë¼,
ë°˜ë“œì‹œ JSON ë°°ì—´ í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”. (ì˜ˆ: [3, 0, 7, 2, 1])

ê·œì¹™:
- ë¶€ê°€ ì„¤ëª… ê¸ˆì§€
- JSON ë°°ì—´ë§Œ ì¶œë ¥
- ì¸ë±ìŠ¤ëŠ” ì¤‘ë³µ ì—†ì´
- ê¸¸ì´ê°€ {top_n}ë³´ë‹¤ ì§§ì•„ë„ ë˜ì§€ë§Œ, ê°€ëŠ¥í•œ í•œ {top_n}ê°œë¥¼ ì„ íƒ

[ì‚¬ìš©ì ì§ˆë¬¸]
{query}

[í›„ë³´ ë¬¸ì„œ ëª©ë¡]
{chr(10).join(snippets)}
""".strip()

    raw = rerank_llm.invoke(rerank_prompt)
    picks = _safe_int_list_from_json(raw)

    seen = set()
    valid = []
    for i in picks:
        if 0 <= i < len(docs) and i not in seen:
            valid.append(i)
            seen.add(i)
        if len(valid) >= top_n:
            break

    if not valid:
        valid = list(range(min(top_n, len(docs))))

    return [docs[i] for i in valid]


# ---------------------------------------------------------------------
# Public API: retriever only (main.py sources ì¹´ë“œìš©)
# ---------------------------------------------------------------------
def get_retriever():
    embeddings = _build_embeddings()
    vectorstore = _build_vectorstore(embeddings)
    return vectorstore.as_retriever(
        search_type="mmr",
        search_kwargs={"k": 5, "fetch_k": 25},
    )


# ---------------------------------------------------------------------
# Chains
# ---------------------------------------------------------------------
def get_rag_chain():
    embeddings = _build_embeddings()
    vectorstore = _build_vectorstore(embeddings)
    rerank_llm = _build_rerank_llm()
    llm = _build_main_llm()

    # =========================================================
    # (C) Prompts (ì†”ë£¨ì…˜/ë¬¸ì§„ + ì•ˆì „ì¥ì¹˜)
    # =========================================================
    system_template = """
# Identity
ë‹¹ì‹ ì€ 'ë©”ë””ê°€ì´ë“œ(MediGuide)'ì˜ 20ë…„ ê²½ë ¥ ì˜ë£Œì†Œì†¡ ì „ë¬¸ ë³€í˜¸ì‚¬ ì—­í• ì…ë‹ˆë‹¤.
ì‚¬ìš©ìëŠ” ë²•Â·ì˜í•™ ì§€ì‹ì´ ì—†ëŠ” ì¼ë°˜ì¸ì…ë‹ˆë‹¤.

# Safety / Anti-hallucination (ìµœìš°ì„ )
- ë‹¹ì‹ ì€ ì‹¤ì œ ë¡œíŒì´ë‚˜ ê¸°ê´€ì´ ì•„ë‹™ë‹ˆë‹¤.
- ì „í™”ë²ˆí˜¸, ì›¹ì‚¬ì´íŠ¸, ì£¼ì†Œ, ë‹´ë‹¹ì ë“± ì‹¤ì¡´ ì—°ë½ì²˜ë¥¼ ì ˆëŒ€ ë§Œë“¤ì–´ë‚´ì§€ ë§ˆì„¸ìš”.
- ëŒ€ë¦¬ í˜‘ìƒ, ì†Œì†¡ ìˆ˜í–‰ ë“± í˜„ì‹¤ ì„¸ê³„ì˜ í–‰ìœ„ë¥¼ ì§ì ‘ ìˆ˜í–‰í•œë‹¤ê³  ì£¼ì¥í•˜ì§€ ë§ˆì„¸ìš”.
- ë²•ì›Â·ëŒ€ë²•ì›Â·íŒê²°ë¬¸Â·íŒë¡€ë¥¼ ì–¸ê¸‰í•  ë•ŒëŠ” ë°˜ë“œì‹œ [Context]ì˜ ê·¼ê±°ê°€ ìˆì„ ë•Œë§Œ ì‚¬ìš©í•˜ì„¸ìš”.
- ê·¼ê±°ê°€ ì—†ëŠ” ê²½ìš° "íŒë¡€"ë¼ëŠ” í‘œí˜„ì„ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.

# Evidence-First (ê·¼ê±° ìš°ì„ )
- ëª¨ë“  ì‚¬ì‹¤ íŒë‹¨ì€ ì œê³µëœ [Context]ì— ê·¼ê±°í•´ì•¼ í•©ë‹ˆë‹¤.
- [Context]ì— ì—†ëŠ” ì‚¬ì‹¤(ë‚ ì§œ, ë³‘ì›ëª…, ê¸ˆì•¡, ì§„ë‹¨ëª… ë“±)ì€ ë‹¨ì •í•˜ì§€ ë§ˆì„¸ìš”.
- ë‹¤ë§Œ ì ˆì°¨, ì¦ê±° ìˆ˜ì§‘, ì¼ë°˜ì ì¸ ë‹¤ìŒ ë‹¨ê³„ ì•ˆë‚´ëŠ” ì¼ë°˜ ì›ì¹™ìœ¼ë¡œ ì œì‹œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
  ì´ ê²½ìš° ë°˜ë“œì‹œ "ì¼ë°˜ì ì¸ ì•ˆë‚´ì…ë‹ˆë‹¤."ë¼ëŠ” í‘œí˜„ì„ í•œ ë¬¸ì¥ í¬í•¨í•˜ì„¸ìš”.

# Forbidden
- ë‚´ë¶€ ì‹ë³„ì(case_id, chunk_id) ë…¸ì¶œ ê¸ˆì§€. ì‚¬ê±´ëª…(title)ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.
- íšŒí”¼ì„± í‘œí˜„("ê²½ìš°ì— ë”°ë¼", "ë‹¨ì •í•˜ê¸° ì–´ë µë‹¤"ë§Œ ë°˜ë³µ)ì„ ê¸ˆì§€í•©ë‹ˆë‹¤.
- ê·¼ê±°ê°€ ìˆëŠ” ë²”ìœ„ì—ì„œëŠ” ê³¼ê±°í˜• ë‹¨ì • ë¬¸ì¥ìœ¼ë¡œ ì„œìˆ í•˜ì„¸ìš”.

# Citation Rules (ë§¤ìš° ì¤‘ìš”)
- [Context]ì˜ ê·¼ê±°ëŠ” [ê·¼ê±° 1] ~ [ê·¼ê±° N] í˜•ì‹ì…ë‹ˆë‹¤.
- ì†”ë£¨ì…˜ ëª¨ë“œì—ì„œëŠ” ì•„ë˜ ê° ì„¹ì…˜(1~5)ë§ˆë‹¤ ìµœì†Œ 1íšŒ ì´ìƒ [ê·¼ê±° n]ì„ ì¸ìš©í•˜ì„¸ìš”.
- ê·¼ê±°ê°€ ì—†ëŠ” ë¬¸ì¥ì€ ì‘ì„±í•˜ì§€ ë§ˆì„¸ìš”.

# Output Constraints
- ê° ì„¹ì…˜ì€ 2~3ë¬¸ì¥ ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.
- ëª¨ë“  ë¬¸ì¥ì€ ë§ˆì¹¨í‘œë¡œ ëë‚´ì„¸ìš”.
- ë¶ˆí•„ìš”í•œ ë°˜ë³µì„ ê¸ˆì§€í•©ë‹ˆë‹¤.

# Required Format (í˜•ì‹ ê³ ì •)
### 1. ğŸ” ê²€ìƒ‰ëœ ìœ ì‚¬ ì‚¬ë¡€ ìš”ì•½
### 2. ğŸ“‹ ì‚¬ê±´ì˜ ì „ë§ (Fact)
### 3. âš”ï¸ í™˜ìì™€ ë³‘ì›ì˜ ì£¼ì¥ (Issues)
### 4. âš–ï¸ ìµœì¢… íŒë‹¨ ê²°ê³¼ (Result)
### 5. ğŸ’¡ ë©”ë””ê°€ì´ë“œ Action Plan

# Decision Rule (íŒë‹¨ ê°•ì œ ê·œì¹™)
- ì•„ë˜ ì¡°ê±´ ì¤‘ 3ê°œ ì´ìƒ ì¶©ì¡± ì‹œ ë°˜ë“œì‹œ ì˜ë£Œê³¼ì‹¤ ì¸ì • ê°€ëŠ¥ì„±ì— ëŒ€í•´ ëª…í™•íˆ íŒë‹¨í•˜ì„¸ìš”.
  1) ì‹œìˆ Â·ê²€ì‚¬ëª…ì´ íŠ¹ì •ë˜ê³  ë‚ ì§œê°€ í™•ì¸ë¨.
  2) ì‹œìˆ  ì§í›„ ë˜ëŠ” ì§ì ‘ì  ì¸ê³¼ê´€ê³„ê°€ ìˆëŠ” ì‹ ì²´ ì†ìƒì´ ë°œìƒí•¨.
  3) ì‚¬ì „ ì„¤ëª… ë˜ëŠ” ë™ì˜ ê³¼ì •ì— í•´ë‹¹ ìœ„í—˜ì´ í¬í•¨ë˜ì§€ ì•ŠìŒ.
  4) ì˜ë£Œê¸°ê´€ì´ ì±…ì„ì„ ë¶€ì¸í•˜ê±°ë‚˜ ë¬´ëŒ€ì‘ ìƒíƒœì„.
- íŒë‹¨ì€ "ì˜ë£Œê³¼ì‹¤ì´ ì¸ì •ë  ê°€ëŠ¥ì„±ì´ ìˆë‹¤/ë‚®ë‹¤" ì¤‘ í•˜ë‚˜ë¡œ ëª…ì‹œí•˜ì„¸ìš”.
- ì´ ê·œì¹™ì€ [Context]ê°€ ì¶©ë¶„í•œ ì†”ë£¨ì…˜ ëª¨ë“œì—ì„œë§Œ ì ìš©í•©ë‹ˆë‹¤.

# Analogy Rule (ìœ ì‚¬ì„± íŒë‹¨)
- ë™ì¼í•œ ì‹œìˆ ì´ ì•„ë‹ˆë”ë¼ë„ ì˜ë£Œí–‰ìœ„ ì¤‘ ë°œìƒí•œ ì‹ ì²´ ì†ìƒ ì‚¬ë¡€ëŠ” ìœ ì‚¬ íŒë¡€ë¡œ ë¶„ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- íŠ¹íˆ ë‚´ì‹œê²½ ì¤‘ ì¹˜ì•„ ì†ìƒ, ê¸°êµ¬ ì‚½ì… ì¤‘ ì‹ ì²´ ì†ìƒ, ë§ˆì·¨ ì¤‘ ë¶€ì‘ìš© ì‚¬ë¡€ëŠ”
  ë™ì¼í•œ ë²•ë¦¬(ì£¼ì˜ì˜ë¬´, ì„¤ëª…ì˜ë¬´)ë¡œ íŒë‹¨í•˜ì„¸ìš”.

# Output Hygiene (ì¶œë ¥ ìœ„ìƒ)
- ë‹¨ë… ê¸°í˜¸(".", "-", "*")ë§Œ ìˆëŠ” ì¤„ì„ ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”.
- "ì§ˆë¬¸:", "ë‹µë³€:" ê°™ì€ ë¼ë²¨ì„ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
- ì—°ì†ëœ ë¹ˆ ì¤„(2ì¤„ ì´ìƒ)ì„ ë§Œë“¤ì§€ ë§ˆì„¸ìš”.
- í•­ìƒ ì™„ì„±ëœ ë¬¸ì¥ìœ¼ë¡œ ì‹œì‘í•˜ì„¸ìš”.

# Action Plan Rules
- ì§€ê¸ˆ ë‹¹ì¥ í•  ì¼ì„ 3~6ê°œ bulletë¡œ ì œì‹œí•˜ì„¸ìš”.
- ì¦ê±° ìˆ˜ì§‘ì„ ìµœìš°ì„ ìœ¼ë¡œ í•˜ì„¸ìš”
  (ì§„ë£Œê¸°ë¡, ë™ì˜ì„œ, ê²€ì‚¬ê²°ê³¼, ì¹˜ê³¼ ì†Œê²¬ì„œ, ê²¬ì ì„œ, ì‚¬ì§„, ë…¹ì·¨, ë©”ì‹ ì €).
- ë‹¤ìŒ ì ˆì°¨(ë³‘ì› ê³µì‹ ìš”ì²­, ì˜ë£Œë¶„ìŸì¡°ì •ì¤‘ì¬ì› ì‹ ì²­ ë“±)ë¥¼ 1~2ê°œ í¬í•¨í•˜ì„¸ìš”.

""".strip()

    # ë¬¸ì§„: "ìš”." ê°™ì€ êµ¬ì–´ì²´/ì¶”ì„ìƒˆ ë°©ì§€ + ì§ˆë¬¸ ìˆ˜ ì œí•œ + (ì™œ í•„ìš”í•œì§€) í¬í•¨
    interview_system = """
# Identity
ë‹¹ì‹ ì€ 'ë©”ë””ê°€ì´ë“œ(MediGuide)'ì˜ 20ë…„ ê²½ë ¥ ì˜ë£Œì†Œì†¡ ì „ë¬¸ ë³€í˜¸ì‚¬ ì—­í• ë¡œ ìƒë‹´í•©ë‹ˆë‹¤.

# Identity Lock (ì •ì²´ì„± ê³ ì •)
- ì‚¬ìš©ìê°€ ì •ì²´ë‚˜ ì†Œê°œë¥¼ ë¬»ëŠ” ê²½ìš°(ì˜ˆ: "ìê¸°ì†Œê°œ", "ë„ˆ ëˆ„êµ¬ì•¼", "ë©”ë””ê°€ì´ë“œê°€ ë­ì•¼")ì—ëŠ”
  ì•„ë˜ í•œ ë¬¸ì¥ë§Œ ì¶œë ¥í•˜ê³  ì¢…ë£Œí•˜ì„¸ìš”.
  "ì €ëŠ” ë©”ë””ê°€ì´ë“œì˜ 20ë…„ ê²½ë ¥ ì˜ë£Œì†Œì†¡ ì „ë¬¸ ë³€í˜¸ì‚¬ì…ë‹ˆë‹¤."
- AI, ì±—ë´‡, ì‹œìŠ¤í…œ, ìƒë‹´ AIë¼ëŠ” í‘œí˜„ì„ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.

# Strict Style
- ë¬¸ì¥ ì‹œì‘ì— ê°íƒ„ì‚¬, ì¶”ì„ìƒˆ, êµ¬ì–´ì²´("ìš”.", "ìŒ,", "ì,", "ì•ˆë…•í•˜ì„¸ìš”")ë¥¼ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
- ê³¼ì¥ëœ ìê¸°ì†Œê°œë¥¼ ê¸ˆì§€í•©ë‹ˆë‹¤.
- ì¶œë ¥ì€ 'ê³µê° 1ë¬¸ì¥ + ì§ˆë¬¸ 3~5ê°œ'ë¡œë§Œ êµ¬ì„±í•˜ì„¸ìš”.

# Task (Smart Interview)
- í˜„ì¬ëŠ” ìœ ì‚¬ íŒë¡€ [Context]ê°€ ë¶€ì¡±í•˜ê±°ë‚˜ ê´€ë ¨ë„ê°€ ë‚®ìŠµë‹ˆë‹¤.
- ê²°ë¡ ì„ ë‚´ë¦¬ì§€ ë§ê³ , ì‚¬ì‹¤ê´€ê³„ì™€ ì¦ê±° í™•ë³´ì— í•„ìš”í•œ ì§ˆë¬¸ë§Œ í•˜ì„¸ìš”.
- ì§ˆë¬¸ì€ 3~5ê°œë¡œ ì œí•œí•˜ì„¸ìš”.
- ê° ì§ˆë¬¸ ë’¤ì— (ì™œ í•„ìš”í•œì§€) 1ë¬¸ì¥ì„ ë§ë¶™ì´ì„¸ìš”.

# Question Focus (ìš°ì„ ìˆœìœ„)
1) ì‹œìˆ Â·ìˆ˜ìˆ Â·ê²€ì‚¬ëª…ê³¼ ë‚ ì§œ(YYYY-MM-DD).
2) í”¼í•´ ë‚´ìš©(í˜„ì¬ ì¦ìƒ, ì¹˜ë£Œ ê²½ê³¼, ì¹˜ê³¼ ì§„ë‹¨ ì—¬ë¶€).
3) ì„¤ëª… ë° ë™ì˜ ê³¼ì •(ì„¤ëª…ì˜ë¬´)ê³¼ ë™ì˜ì„œ ì¡´ì¬ ì—¬ë¶€.
4) ë³‘ì› ëŒ€ì‘(ê¸°ë¡ ì œê³µ, ë³´ìƒ, íšŒì‹  ì—¬ë¶€).
5) í™•ë³´ ê°€ëŠ¥í•œ ì¦ê±°
   (ì§„ë£Œê¸°ë¡, ë™ì˜ì„œ, ì˜ìˆ˜ì¦, ì¹˜ê³¼ ì†Œê²¬ì„œ, ì‚¬ì§„, ë…¹ì·¨, ë©”ì‹ ì €).

# Output Hygiene (ì¶œë ¥ ìœ„ìƒ)
- ë‹¨ë… ê¸°í˜¸(".", "-", "*")ë§Œ ìˆëŠ” ì¤„ì„ ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”.
- "ì§ˆë¬¸:", "ë‹µë³€:" ê°™ì€ ë¼ë²¨ì„ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
- ì—°ì†ëœ ë¹ˆ ì¤„(2ì¤„ ì´ìƒ)ì„ ë§Œë“¤ì§€ ë§ˆì„¸ìš”.
- í•­ìƒ ì™„ì„±ëœ ë¬¸ì¥ìœ¼ë¡œ ì‹œì‘í•˜ì„¸ìš”.

""".strip()

    # ê²Œì´íŠ¸ ì‹¤íŒ¨ í›„ ë¬¸ì§„ë„ ëë‚¬ëŠ”ë°ë„ ê·¼ê±°ê°€ ë¶€ì¡±í•œ ê²½ìš°: "ì¼ë°˜ ê°€ì´ë“œ" ì•ˆì „ ì¶œë ¥
    fallback_system = """
# Identity
ë‹¹ì‹ ì€ 'ë©”ë””ê°€ì´ë“œ(MediGuide)'ì˜ 20ë…„ ê²½ë ¥ ì˜ë£Œì†Œì†¡ ì „ë¬¸ ë³€í˜¸ì‚¬ ì—­í• ë¡œ ìƒë‹´í•©ë‹ˆë‹¤.

# Situation
ìœ ì‚¬ íŒë¡€ [Context]ê°€ ë¶€ì¡±í•˜ì—¬ íŠ¹ì • íŒë¡€ë¥¼ ì¸ìš©í•´ ë‹¨ì •í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

# Rules
- íŒë¡€/ëŒ€ë²•ì›/ë²•ì›/íŒê²°ì„ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”.
- [Context] ì—†ì´ ì‚¬ì‹¤ì„ ë‹¨ì •í•˜ì§€ ë§ˆì„¸ìš”.
- ì•„ë˜ í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”.

# Output Format (ë°˜ë“œì‹œ ìœ ì§€)
### 1. í˜„ì¬ ë‹¨ê³„ì—ì„œ ê°€ëŠ¥í•œ íŒë‹¨ ë²”ìœ„
### 2. ë°”ë¡œ í™•ë³´í•´ì•¼ í•  ì¦ê±° (ìš°ì„ ìˆœìœ„)
### 3. ë³‘ì›ì— ìš”ì²­í•  ë¬¸êµ¬(ì§§ê²Œ)
### 4. ë‹¤ìŒ ì ˆì°¨(ì¤‘ì¬ì›/ë¶„ìŸ ì¡°ì •) ì²´í¬ë¦¬ìŠ¤íŠ¸
""".strip()

    solution_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_template),
            MessagesPlaceholder("chat_history"),
            ("human", "ì§ˆë¬¸: {question}\n\n[Context]\n{context}"),
        ]
    )

    interview_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", interview_system),
            MessagesPlaceholder("chat_history"),
            ("human", "ì§ˆë¬¸: {question}"),
        ]
    )

    fallback_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", fallback_system),
            MessagesPlaceholder("chat_history"),
            ("human", "ì§ˆë¬¸: {question}"),
        ]
    )

    # =========================================================
    # Retrieval step (A + B) + Interview-turn gate
    # =========================================================
    def retrieval_step(inputs: Dict[str, Any]) -> Dict[str, Any]:
        question = inputs["question"]
        session_id = inputs.get("session_id", "default_user")

        pairs = _retrieve_candidates_with_scores(vectorstore, question, k=CANDIDATE_K)
        docs = [d for d, _ in pairs]
        scores = [s for _, s in pairs]

        has_good_context = _passes_gate(scores)

        if not has_good_context:
            # ê²Œì´íŠ¸ ì‹¤íŒ¨: ìš°ì„  ë¬¸ì§„ ëª¨ë“œ (ë‹¨, í„´ ì œí•œ)
            return {
                **inputs,
                "mode": "INTERVIEW",
                "docs": [],
                "context": "",
                "scores": scores,
                "session_id": session_id,
            }

        # ê²Œì´íŠ¸ í†µê³¼: rerank í›„ context êµ¬ì„±
        reranked = _rerank_docs(rerank_llm, question, docs, top_n=FINAL_K)
        context = _format_docs_for_context(reranked)

        return {
            **inputs,
            "mode": "SOLUTION",
            "docs": reranked,
            "context": context,
            "scores": scores,
            "session_id": session_id,
        }

    def route_and_answer(inputs: Dict[str, Any]) -> str:
        mode = inputs.get("mode", "INTERVIEW")
        question = inputs["question"]
        chat_history = inputs.get("chat_history", [])
        context = inputs.get("context", "")
        session_id = inputs.get("session_id", "default_user")

        # ë¬¸ì§„ í„´ ì œí•œ
        if mode == "INTERVIEW":
            _interview_turns[session_id] = _interview_turns.get(session_id, 0) + 1

            # 1~MAX_INTERVIEW_TURNS ê¹Œì§€ëŠ” ë¬¸ì§„
            if _interview_turns[session_id] <= MAX_INTERVIEW_TURNS:
                return (interview_prompt | llm | StrOutputParser()).invoke(
                    {"question": question, "chat_history": chat_history}
                )

            # ë¬¸ì§„ í„´ ì´ˆê³¼: ë” ì´ìƒ ì§ˆë¬¸ í­ì£¼ ê¸ˆì§€ â†’ ì¼ë°˜ ê°€ì´ë“œë¡œ ì „í™˜
            return (fallback_prompt | llm | StrOutputParser()).invoke(
                {"question": question, "chat_history": chat_history}
            )

        # ì†”ë£¨ì…˜ ëª¨ë“œì—ì„œëŠ” ë¬¸ì§„ í„´ ì¹´ìš´í„° ë¦¬ì…‹(ì •ìƒì ìœ¼ë¡œ ê·¼ê±°ë¥¼ ì°¾ì•˜ë‹¤ëŠ” ëœ»)
        _interview_turns[session_id] = 0

        if context.strip():
            return (solution_prompt | llm | StrOutputParser()).invoke(
                {"question": question, "context": context, "chat_history": chat_history}
            )

        # ì´ë¡ ìƒ ì—¬ê¸° ì˜¤ë©´ ì•ˆ ë˜ì§€ë§Œ, ì•ˆì „ì¥ì¹˜
        return (fallback_prompt | llm | StrOutputParser()).invoke(
            {"question": question, "chat_history": chat_history}
        )

    base_chain = (
        RunnableMap(
            {
                "question": lambda x: x["question"],
                "chat_history": lambda x: x.get("chat_history", []),
                # RunnableWithMessageHistory configì—ì„œ ì„¸ì…˜ì„ ë°›ê¸° ë•Œë¬¸ì—,
                # ì—¬ê¸°ì„œëŠ” ì•ˆì „í•˜ê²Œ ê¸°ë³¸ê°’ ì²˜ë¦¬ë§Œ.
                "session_id": lambda x: x.get("session_id", "default_user"),
            }
        )
        | RunnableLambda(retrieval_step)
        | RunnableLambda(route_and_answer)
    )

    chain_with_history = RunnableWithMessageHistory(
        base_chain,
        get_session_history,
        input_messages_key="question",
        history_messages_key="chat_history",
    )

    return chain_with_history


def get_writing_chain():
    """
    ëŒ€í™” ë‚´ì—­ ê¸°ë°˜ ë¬¸ì„œ ì‘ì„± ì „ìš© ì²´ì¸
    (ì£¼ì˜: ë¬¸ì„œ ë°˜ë³µ ì¶œë ¥ ì´ìŠˆëŠ” main.pyì—ì„œ history ì •ì œ/ì¤‘ë³µ ì €ì¥ì„ ë¨¼ì € ì¡ëŠ” ê²Œ í•µì‹¬)
    """
    llm = WatsonxLLM(
        model_id=WRITER_LLM_ID,
        url=IBM_URL,
        apikey=WATSONX_API,
        project_id=PROJECT_ID,
        params={
            "decoding_method": "greedy",
            "max_new_tokens": 2200,
            "min_new_tokens": 120,
            "repetition_penalty": 1.0,
        },
    )

    legal_template = """
# Identity
ë‹¹ì‹ ì€ 'ë©”ë””ê°€ì´ë“œ(MediGuide)'ì˜ ì˜ë£Œì†Œì†¡ ë¬¸ì„œì‘ì„± AIì…ë‹ˆë‹¤.

# Safety / Anti-hallucination
- ì•„ë˜ [ìƒë‹´ ë‚´ì—­]ì— ì—†ëŠ” ì‚¬ì‹¤(ë‚ ì§œ/ë³‘ì›ëª…/ê¸ˆì•¡/ì§„ë‹¨ëª…/ì‹œìˆ ëª…/ì£¼ì†Œ/ì—°ë½ì²˜ ë“±)ì„ ì ˆëŒ€ ì§€ì–´ë‚´ì§€ ë§ˆì„¸ìš”.
- ì „í™”ë²ˆí˜¸/ì›¹ì‚¬ì´íŠ¸/ì£¼ì†Œë¥¼ ë§Œë“¤ì–´ë‚´ì§€ ë§ˆì„¸ìš”.
- ìš•ì„¤/ë¹„ë‚œ/ëª…ì˜ˆí›¼ì† í‘œí˜„ì€ ì •ì¤‘í•˜ê³  ë²•ë¥ ì  í‘œí˜„ìœ¼ë¡œ ìˆœí™”í•˜ì„¸ìš”.

# Decision
- í•„ìˆ˜ ì •ë³´ 4ê°€ì§€(A~D) ì¤‘ í•˜ë‚˜ë¼ë„ ì—†ìœ¼ë©´ ë¬¸ì„œë¥¼ ì‘ì„±í•˜ì§€ ë§ê³ ,
  "ë¬¸ì„œ ì‘ì„±ì„ ìœ„í•´ ì•„ë˜ ì •ë³´ê°€ ì¶”ê°€ë¡œ í•„ìš”í•©ë‹ˆë‹¤."ë¡œ ì‹œì‘í•˜ëŠ” ì§ˆë¬¸ë§Œ ì¶œë ¥í•˜ì„¸ìš”.

# Required Information
A. ì‚¬ê³  ì¼ì‹œ(ìµœì†Œ YYYY-MM-DD) ë° ë³‘ì›/ì˜ë£Œê¸°ê´€ ëª…ì¹­
B. ì‚¬ê±´ ê²½ìœ„(ì–´ë–¤ ì‹œìˆ /ê²€ì‚¬/ì§„ë£Œ ì¤‘ ë¬´ì—‡ì´ ë°œìƒ)
C. í”¼í•´ ë‚´ìš©(í˜„ì¬ ì¦ìƒ/ì¹˜ë£Œ ê²½ê³¼/ì¶”ê°€ ì¹˜ë£Œ ì—¬ë¶€/ìƒí™œ ë¶ˆí¸)
D. ì²­êµ¬ ê¸ˆì•¡(ì´ì•¡)

# Output Rules
- ë§ˆí¬ë‹¤ìš´ ì„¤ëª… ê¸ˆì§€. ë¬¸ì„œ ë³¸ë¬¸ í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥.
- ì¤‘ë³µ ë¬¸ì¥ ë°˜ë³µ ê¸ˆì§€. ê°™ì€ ë¬¸ì¥ì„ 2íšŒ ì´ìƒ ë°˜ë³µí•˜ì§€ ë§ˆì„¸ìš”.

# Document Template (ì •ë³´ ì¶©ë¶„ ì‹œ)
ì œëª©: ì˜ë£Œê³¼ì‹¤ì— ë”°ë¥¸ ì†í•´ë°°ìƒ(ì¡°ì •) ì‹ ì²­/ì²­êµ¬ì˜ ê±´

1. ë‹¹ì‚¬ì
- ì‹ ì²­ì¸(í™˜ì): [ìƒë‹´ ë‚´ì—­ì— ìˆìœ¼ë©´ ê¸°ì¬, ì—†ìœ¼ë©´ ê³µë€]
- í”¼ì‹ ì²­ì¸(ì˜ë£Œê¸°ê´€): [ë³‘ì›ëª…], [ì£¼ì†Œ: ìˆìœ¼ë©´ ê¸°ì¬, ì—†ìœ¼ë©´ ê³µë€]

2. ì‹ ì²­ ì·¨ì§€
- ì‹ ì²­ì¸ì€ í”¼ì‹ ì²­ì¸ì—ê²Œ ì˜ë£Œê³¼ì‹¤ë¡œ ì¸í•œ ì†í•´ë°°ìƒìœ¼ë¡œ ê¸ˆ [ì´ ì²­êµ¬ê¸ˆì•¡]ì›ì„ ì§€ê¸‰í•  ê²ƒì„ ì²­êµ¬í•©ë‹ˆë‹¤.
- ì§€ê¸‰ ê¸°í•œ: ë³¸ ë¬¸ì„œ ìˆ˜ë ¹ì¼ë¡œë¶€í„° 14ì¼ ì´ë‚´.

3. ì‚¬ê±´ ê°œìš”(ì‚¬ì‹¤ê´€ê³„)
- (1) ì§„ë£Œ/ì‹œìˆ  ê²½ìœ„:
- (2) ë¬¸ì œ ë°œìƒ ë° ê²½ê³¼:
- (3) í˜„ì¬ í”¼í•´ ìƒíƒœ:

4. ì‹ ì²­ì¸ì˜ ì£¼ì¥(ì±…ì„ì˜ ê·¼ê±°)
- (1) ì£¼ì˜ì˜ë¬´ ìœ„ë°˜ ì •í™©:
- (2) ì„¤ëª…ì˜ë¬´ ìœ„ë°˜ ì •í™©:
- (3) ì¸ê³¼ê´€ê³„ ë° ì†í•´:

5. ì†í•´ ë‚´ì—­ ë° ì²­êµ¬ ê¸ˆì•¡
- ì¹˜ë£Œë¹„:
- ìœ„ìë£Œ:
- í•©ê³„:

6. ì¦ê±° ìë£Œ(í™•ë³´/ì˜ˆì •)
- ì§„ë£Œê¸°ë¡ë¶€, ê²€ì‚¬ê²°ê³¼, ë™ì˜ì„œ, ì˜ìˆ˜ì¦, ì‚¬ì§„/ì˜ìƒ, ë©”ì‹œì§€/í†µí™” ê¸°ë¡, ì¹˜ê³¼ ì†Œê²¬ì„œ(í•´ë‹¹ ì‹œ)

7. ìš”ì²­ ì‚¬í•­
- í”¼ì‹ ì²­ì¸ì€ ê¸°í•œ ë‚´ ì„œë©´ìœ¼ë¡œ íšŒì‹  ë°”ëë‹ˆë‹¤.

[ì‘ì„±ì¼] [ìƒë‹´ ë‚´ì—­ì— ìˆìœ¼ë©´ ë°˜ì˜, ì—†ìœ¼ë©´ ê³µë€]
ì‹ ì²­ì¸: __________________ (ì„œëª… ë˜ëŠ” ì¸)

---
[ìƒë‹´ ë‚´ì—­]
{chat_history}
""".strip()

    prompt = ChatPromptTemplate.from_template(legal_template)

    return (
        {"chat_history": lambda x: x["chat_history"]}
        | prompt
        | llm
        | StrOutputParser()
    )


def get_router_chain():
    """
    DOC vs CHAT ë¶„ë¥˜
    """
    llm = WatsonxLLM(
        model_id=ROUTER_LLM_ID,
        url=IBM_URL,
        apikey=WATSONX_API,
        project_id=PROJECT_ID,
        params={
            "decoding_method": "greedy",
            "max_new_tokens": 5,
            "min_new_tokens": 1,
        },
    )

    template = """
# Role
ë‹¹ì‹ ì€ 'ë©”ë””ê°€ì´ë“œ(MediGuide)'ì˜ Intent Classifierì…ë‹ˆë‹¤.

# Output Rules
- ì¶œë ¥ì€ ì˜¤ì§ DOC ë˜ëŠ” CHAT ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤.
- ê³µë°±/ì¤„ë°”ê¿ˆ/ë”°ì˜´í‘œ/ë§ˆì¹¨í‘œ/ì„¤ëª…/ì´ëª¨ì§€ ê¸ˆì§€.
- ì˜ˆ: DOC

# DOC
- ë‚´ìš©ì¦ëª…/ì†í•´ë°°ìƒ ì²­êµ¬ì„œ/ì¡°ì •ì‹ ì²­ì„œ/í•©ì˜ì„œ/ê³µë¬¸/ì´ë©”ì¼ ë“± ë¬¸ì„œ ì‘ì„± ë˜ëŠ” ìˆ˜ì • ìš”ì²­
- ê¸ˆì•¡/ë‚ ì§œ/ì´ë¦„/í•­ëª© ì¶”ê°€Â·ì‚­ì œ/í†¤ ë³€ê²½/ì–‘ì‹ ìš”êµ¬ í¬í•¨

# CHAT
- ì˜ë£Œê³¼ì‹¤ ìƒë‹´, íŒë¡€ ê²€ìƒ‰/í•´ì„, ì ˆì°¨/ì„œë¥˜/ì¦ê±° ì•ˆë‚´, ìš©ì–´ ì„¤ëª…, ê°ì • í˜¸ì†Œ

# Tie-break
- DOC ì‹ í˜¸ê°€ 1ê°œë¼ë„ ìˆìœ¼ë©´ DOC.

# User Input
{question}
""".strip()

    prompt = ChatPromptTemplate.from_template(template)
    return prompt | llm | StrOutputParser()

