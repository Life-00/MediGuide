import os, re, shutil
import pandas as pd
from dotenv import load_dotenv
from langchain_core.documents import Document
from langchain_ibm import WatsonxEmbeddings
from langchain_chroma import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter
from ibm_watsonx_ai.metanames import EmbedTextParamsMetaNames

load_dotenv()

IBM_URL = os.getenv("IBM_CLOUD_URL")
PROJECT_ID = os.getenv("PROJECT_ID")
WATSONX_API = os.getenv("API_KEY")
PERSIST_DIR = "./chroma_db_fixed"
COLLECTION_NAME = "mediguide_cases"

def normalize_text(x: str) -> str:
    if x is None:
        return ""
    x = str(x).replace("\r\n", "\n").replace("\r", "\n")
    x = re.sub(r"[ \t]+", " ", x)
    x = re.sub(r"\n{3,}", "\n\n", x)
    return x.strip()

def ingest_data():
    print("ğŸ“‚ ë°ì´í„° ë¡œë”© ë° DB êµ¬ì¶• ì‹œì‘...")

    file_path = "test-data2.xlsx"
    try:
        df = pd.read_excel(file_path)
    except FileNotFoundError:
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        return

    # ë¶ˆí•„ìš” ì»¬ëŸ¼ ì œê±°
    df = df.drop(columns=[c for c in df.columns if c.startswith("Unnamed")], errors="ignore")

    # í…ìŠ¤íŠ¸ splitter (ëŒ€ëµì ì¸ ê¸¸ì´ ê¸°ì¤€, ìƒí™©ì— ë§ê²Œ ì¡°ì ˆ ê°€ëŠ¥)
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=900,      # ì„ë² ë”© 512 í† í° truncateë¥¼ ê³ ë ¤í•´ ë„‰ë„‰íˆ ìª¼ê°¬
        chunk_overlap=150,
        separators=["\n\n", "\n", ". ", " "]
    )

    docs = []
    print(f"ğŸ”¹ ì´ {len(df)}ê°œì˜ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.")

    for _, row in df.iterrows():
        case_id = str(row.get("case_id", "unknown"))
        dept = str(row.get("medical_dept", "unknown"))
        title = str(row.get("title", "N/A"))
        seq = str(row.get("seq", ""))

        sections = {
            "overview": row.get("case_overview", ""),
            "issues": row.get("issues", ""),
            "solution": row.get("solution", ""),
            "result": row.get("result", ""),
            "final_result": row.get("final_result", ""),
        }

        for section_name, section_text in sections.items():
            section_text = normalize_text(section_text)
            if not section_text:
                continue

            # ì„¹ì…˜ë³„ chunking
            chunks = splitter.split_text(section_text)
            for i, ch in enumerate(chunks):
                content = (
                    f"[ì‚¬ê±´ëª…]: {title}\n"
                    f"[ì§„ë£Œê³¼ëª©]: {dept}\n"
                    f"[ì„¹ì…˜]: {section_name}\n\n"
                    f"{ch}"
                )

                metadata = {
                    "case_id": case_id,
                    "dept": dept,
                    "title": title,         # âœ… ê·¼ê±° ì¹´ë“œìš©
                    "seq": seq,             # âœ… ì›ë¬¸ ë§í¬ ë§¤í•‘ìš©
                    "section": section_name,
                    "chunk_id": f"{case_id}:{section_name}:{i}",
                }

                docs.append(Document(page_content=content, metadata=metadata))

    # ì„ë² ë”© ì„¤ì •
    embed_params = {
        EmbedTextParamsMetaNames.TRUNCATE_INPUT_TOKENS: 512,
        EmbedTextParamsMetaNames.RETURN_OPTIONS: {"input_text": True},
    }

    embeddings = WatsonxEmbeddings(
        model_id="ibm/granite-embedding-278m-multilingual",
        url=IBM_URL,
        project_id=PROJECT_ID,
        params=embed_params,
        apikey=WATSONX_API,
    )

    # DB ì¬ìƒì„±
    if os.path.exists(PERSIST_DIR):
        print("âš ï¸ ê¸°ì¡´ DB í´ë”ê°€ ì¡´ì¬í•©ë‹ˆë‹¤. ë®ì–´ì“°ê¸°ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.")
        shutil.rmtree(PERSIST_DIR)

    Chroma.from_documents(
        documents=docs,
        embedding=embeddings,
        persist_directory=PERSIST_DIR,
        collection_name=COLLECTION_NAME
    )

    print(f"âœ… DB êµ¬ì¶• ì™„ë£Œ! docs={len(docs)} ì €ì¥ ê²½ë¡œ: {PERSIST_DIR}")

if __name__ == "__main__":
    ingest_data()
